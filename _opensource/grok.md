---
title: Grok
homepage: https://x.ai/blog/grok-os
contribute: https://github.com/xai-org/grok-1
image: https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Grok_logo.svg/512px-Grok_logo.svg.png
tags: [X]

caption:
  thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Grok_logo.svg/512px-Grok_logo.svg.png
---

Grok-1 is a 314 billion parameter Mixture-of-Experts model trained from scratch by xAI.
